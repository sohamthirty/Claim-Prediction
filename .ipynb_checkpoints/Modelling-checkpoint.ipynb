{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425fc369",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7fb61",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e0588b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b50cbf",
   "metadata": {},
   "source": [
    "## Import the sampled csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "424b8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og= pd.read_csv('Data/data_og.csv')  \n",
    "df_nm1= pd.read_csv('Data/data_nm1.csv')  \n",
    "df_nm2= pd.read_csv('Data/data_nm2.csv')  \n",
    "df_nm3= pd.read_csv('Data/data_nm3.csv')  \n",
    "df_rus= pd.read_csv('Data/data_rus.csv')  \n",
    "df_ros= pd.read_csv('Data/data_ros.csv')  \n",
    "df_smote= pd.read_csv('Data/data_smote.csv')  \n",
    "df_smoteen= pd.read_csv('Data/data_smoteen.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3cef44",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417b25a",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d79f6576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modellr(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    classifier = LogisticRegression(random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2),round(accuracies.mean()*100,2)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71522c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['LogisticRegression Original','LogisticRegression Near Miss1', 'LogisticRegression Near Miss2','LogisticRegression Near Miss3','LogisticRegression Random UnderSampling','LogisticRegression Random Sampling','LogisticRegression Smote','LogisticRegression Smoteen']\n",
    "\n",
    "allmodels = []\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modellr(data_sample_set[i],names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1702557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used : Logistic Regression\n",
      "\n",
      "                                           Sampling Technique    Accuracy    Precision    Recall    F1 Score    CV Score\n",
      "---------------------------------------  --------------------  ----------  -----------  --------  ----------  ----------\n",
      "LogisticRegression Original                             98.69        0            0         0          98.49       98.49\n",
      "LogisticRegression Near Miss1                           67.24        0.7          0.62      0.66       65.97       65.97\n",
      "LogisticRegression Near Miss2                           79.31        0.87         0.7       0.77       77.91       77.91\n",
      "LogisticRegression Near Miss3                           62.07        0.62         0.64      0.63       65.47       65.47\n",
      "LogisticRegression Random UnderSampling                 71.98        0.75         0.67      0.71       75.47       75.47\n",
      "LogisticRegression Random Sampling                      76.15        0.78         0.73      0.75       76.38       76.38\n",
      "LogisticRegression Smote                                88.06        0.84         0.93      0.89       88.27       88.27\n",
      "LogisticRegression Smoteen                              90.56        0.89         0.94      0.91       90.59       90.59\n"
     ]
    }
   ],
   "source": [
    "print(\"Model used : Logistic Regression\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28db496",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "036f7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelknn(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['KNN Original','KNN Near Miss1', 'KNN Near Miss2','KNN Near Miss3','KNN Random UnderSampling','KNN Random Sampling','KNN Smote','KNN Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelknn(data_sample_set[i],names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf745aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used : KNN\n",
      "\n",
      "Sampling Technique          Accuracy    Precision    Recall    F1 Score    CV Score\n",
      "------------------------  ----------  -----------  --------  ----------  ----------\n",
      "KNN Original                   98.64         0         0           0          98.41\n",
      "KNN Near Miss1                 76.72         0.84      0.67        0.75       77.12\n",
      "KNN Near Miss2                 92.24         0.99      0.86        0.92       88.2\n",
      "KNN Near Miss3                 65.3          0.69      0.57        0.63       66.19\n",
      "KNN Random UnderSampling       70.26         0.71      0.7         0.71       73.09\n",
      "KNN Random Sampling            97.25         0.95      1           0.97       97.12\n",
      "KNN Smote                      94.94         0.92      0.98        0.95       95.07\n",
      "KNN Smoteen                    97.31         0.96      0.99        0.98       97.11\n"
     ]
    }
   ],
   "source": [
    "print(\"Model used : KNN\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98bca7",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58d620e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeldt(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['DecisionTree Original','DecisionTree Near Miss1', 'DecisionTree Near Miss2','DecisionTree Near Miss3','DecisionTree Random UnderSampling','DecisionTree Random Sampling','DecisionTree Smote','DecisionTree Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modeldt(data_sample_set[i],names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a533c300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used : Decision Tree\n",
      "\n",
      "Sampling Technique                   Accuracy    Precision    Recall    F1 Score    CV Score\n",
      "---------------------------------  ----------  -----------  --------  ----------  ----------\n",
      "DecisionTree Original                   97.3          0.06      0.07        0.06       97.07\n",
      "DecisionTree Near Miss1                 79.31         0.8       0.79        0.79       77.99\n",
      "DecisionTree Near Miss2                 93.53         0.94      0.94        0.94       91.87\n",
      "DecisionTree Near Miss3                 62.28         0.64      0.59        0.61       64.1\n",
      "DecisionTree Random UnderSampling       62.72         0.63      0.63        0.63       66.26\n",
      "DecisionTree Random Sampling            98.91         0.98      1           0.99       98.79\n",
      "DecisionTree Smote                      96.89         0.96      0.98        0.97       96.7\n",
      "DecisionTree Smoteen                    98.48         0.98      0.99        0.99       98.38\n"
     ]
    }
   ],
   "source": [
    "print(\"Model used : Decision Tree\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843b1c8",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ece9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelNB(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['NaiveBayes Original','NaiveBayes Near Miss1', 'NaiveBayes Near Miss2','NaiveBayes Near Miss3','NaiveBayes Random UnderSampling','NaiveBayes Random Sampling','NaiveBayes Smote','NaiveBayes Smoteen']\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelNB(data_sample_set[i],names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfc2d0f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used : NaiveBayes\n",
      "\n",
      "Sampling Technique                 Accuracy    Precision    Recall    F1 Score    CV Score\n",
      "-------------------------------  ----------  -----------  --------  ----------  ----------\n",
      "NaiveBayes Original                   82.88         0.05      0.62        0.09       82.42\n",
      "NaiveBayes Near Miss1                 58.19         0.92      0.19        0.32       57.77\n",
      "NaiveBayes Near Miss2                 57.11         0.95      0.16        0.28       56.98\n",
      "NaiveBayes Near Miss3                 59.27         0.58      0.7         0.63       62.01\n",
      "NaiveBayes Random UnderSampling       69.18         0.71      0.66        0.68       71.58\n",
      "NaiveBayes Random Sampling            71.85         0.72      0.72        0.72       71.83\n",
      "NaiveBayes Smote                      84.3          0.78      0.95        0.86       84.37\n",
      "NaiveBayes Smoteen                    87.34         0.84      0.94        0.89       87.37\n"
     ]
    }
   ],
   "source": [
    "print(\"Model used : NaiveBayes\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd15f98",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "db00ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelRF(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['RandomForest Original','RandomForest Near Miss1', 'RandomForest Near Miss2','RandomForest Near Miss3','RandomForest Random UnderSampling','RandomForest Random Sampling','RandomForest Smote','RandomForest Smoteen']\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelRF(data_sample_set[i],names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1055f2e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used : RandomForest\n",
      "\n",
      "Sampling Technique                   Accuracy    Precision    Recall    F1 Score    CV Score\n",
      "---------------------------------  ----------  -----------  --------  ----------  ----------\n",
      "RandomForest Original                   98.38         0.07      0.02        0.03       98.19\n",
      "RandomForest Near Miss1                 81.47         0.84      0.79        0.81       79.64\n",
      "RandomForest Near Miss2                 93.1          0.95      0.91        0.93       92.52\n",
      "RandomForest Near Miss3                 66.38         0.71      0.57        0.63       69.5\n",
      "RandomForest Random UnderSampling       70.04         0.72      0.67        0.69       72.52\n",
      "RandomForest Random Sampling            99.38         0.99      1           0.99       99.31\n",
      "RandomForest Smote                      97.37         0.97      0.98        0.97       97.34\n",
      "RandomForest Smoteen                    98.92         0.99      0.99        0.99       98.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Model used : RandomForest\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de30421",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d9c0491",
   "metadata": {},
   "source": [
    "def modelSVM(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from sklearn.svm import SVC\n",
    "    classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,str(round(accuracy_score(y_test, y_pred)*100,2)) + \"%\",str(round(precision_score(y_test, y_pred,zero_division=0),2)),(str(round(recall_score(y_test, y_pred),2))),(str(round(f1_score(y_test, y_pred),2)))])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['Original','Near Miss1', 'Near Miss2','Near Miss3','Random UnderSampling','Random Sampling','Smote','Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelSVM(data_sample_set[i],names[i])\n",
    "    \n",
    "#allmodels.append([\"Random Forest\",disp])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd307a74",
   "metadata": {},
   "source": [
    "print(\"Model used : SVM\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67ec54",
   "metadata": {},
   "source": [
    "### Kernel SVM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ce98810",
   "metadata": {},
   "source": [
    "def modelKSVM(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from sklearn.svm import SVC\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,str(round(accuracy_score(y_test, y_pred)*100,2)) + \"%\",str(round(precision_score(y_test, y_pred,zero_division=0),2)),(str(round(recall_score(y_test, y_pred),2))),(str(round(f1_score(y_test, y_pred),2)))])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['Original','Near Miss1', 'Near Miss2','Near Miss3','Random UnderSampling','Random Sampling','Smote','Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelKSVM(data_sample_set[i],names[i])\n",
    "    \n",
    "#allmodels.append([\"Random Forest\",disp])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86d066a8",
   "metadata": {},
   "source": [
    "print(\"Model used : Kernel SVM\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d960f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4231ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:20:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohmt\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:21:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "def modelXG(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from xgboost import XGBClassifier\n",
    "    classifier = XGBClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "    \n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['XGBoost Original','XGBoost Near Miss1', 'XGBoost Near Miss2','XGBoost Near Miss3','XGBoost Random UnderSampling','XGBoost Random Sampling','XGBoost Smote','XGBoost Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelXG(data_sample_set[i],names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model used : XGBoost\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137abb3",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelCB(df_t,m):\n",
    "    X = df_t.iloc[:, :-1]\n",
    "    y = df_t.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    #Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train.loc[:,:] = sc.fit_transform(X_train.loc[:,:])\n",
    "    X_test.loc[:,:] = sc.transform(X_test.loc[:,:])\n",
    "\n",
    "    #Training the model\n",
    "    from catboost import CatBoostClassifier\n",
    "    classifier = CatBoostClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    #Predict\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    #Metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "    \n",
    "    disp.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "\n",
    "    allmodels.append([m,round(accuracy_score(y_test, y_pred)*100,2),round(precision_score(y_test, y_pred,zero_division=0),2),round(recall_score(y_test, y_pred),2),(round(f1_score(y_test, y_pred),2)),round(accuracies.mean()*100,2)])\n",
    "\n",
    "#Splitting the dataset into the Training set and Test set\n",
    "#data_sample_set = ['df_og','df_nm1','df_nm2','df_nm3','df_rus','df_ros','df_smote','df_smoteen']\n",
    "data_sample_set = [df_og,df_nm1,df_nm2,df_nm3,df_rus,df_ros,df_smote,df_smoteen]\n",
    "names = ['CatBoost Original','CatBoost Near Miss1', 'CatBoost Near Miss2','CatBoost Near Miss3','CatBoost Random UnderSampling','CatBoost Random Sampling','CatBoost Smote','CatBoost Smoteen' ]\n",
    "\n",
    "disp = []\n",
    "\n",
    "for i in range(0,8):\n",
    "    modelCB(data_sample_set[i],names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model used : CatBoost\\n\")\n",
    "print(tabulate(disp, headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508b61d",
   "metadata": {},
   "source": [
    "## Comparing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8985ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "modelsSortedF1Score = sorted(allmodels, key=itemgetter(-1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 models\\n')\n",
    "print(tabulate(modelsSortedF1Score[0:10], headers=[\"Sampling Technique\", \"Accuracy\", \"Precision\",\"Recall\",\"F1 Score\",\"CV Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983da8b6",
   "metadata": {},
   "source": [
    "**Best Models without Crossvalidation:**\n",
    "\n",
    "RandomForest Random Over Sampling \n",
    "\n",
    "RandomForest Smoteen\n",
    "\n",
    "DecisionTree Random Over Sampling\n",
    "\n",
    "DecisionTree Smoteen\n",
    "\n",
    "CatBoost Smoteen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb00e0",
   "metadata": {},
   "source": [
    "**Best Models with Crossvalidation:**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39149979",
   "metadata": {},
   "source": [
    "## Export the tested model to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a68b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('banglore_home_prices_model.pickle','wb') as f:\n",
    "    pickle.dump(lr_clf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b39c6d",
   "metadata": {},
   "source": [
    "### Export location and column information to a file that will be useful later on in our prediction application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0331188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "'data_columns' : [col.lower() for col in X.columns]\n",
    "}\n",
    "with open(\"columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
